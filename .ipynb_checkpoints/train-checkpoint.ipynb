{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import config\n",
    "from load_data import load_dataset_fn\n",
    "import os\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_initial_weights(fn):\n",
    "    f = np.load(fn)\n",
    "\n",
    "    # Sorting keys by value of numbers\n",
    "    initial_weights = [f[n] for n in sorted(f.files, key=lambda s: int(s[4:]))]\n",
    "\n",
    "    return initial_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #Check for dirs, if not present make them\n",
    "    if not os.path.exists(config.RESULT_DIR):\n",
    "        os.makedirs(config.RESULT_DIR)\n",
    "\n",
    "    \n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Weights and biases\n",
    "        model_params = model.params()\n",
    "\n",
    "\n",
    "        # restore weights\n",
    "        f = \"weights.npz\"\n",
    "        if os.path.exists(f):\n",
    "            initial_weights = load_initial_weights(f)\n",
    "        else:\n",
    "            initial_weights = None\n",
    "\n",
    "        if initial_weights is not None:\n",
    "            assert len(initial_weights) == len(model_params)\n",
    "            assign_ops = [\n",
    "                w.assign(v) for w, v in zip(model_params, initial_weights)\n",
    "            ]\n",
    "\n",
    "        # Input data\n",
    "        tf_train_dataset = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=(config.BATCH_SIZE, config.CNN_IN_HEIGHT, config.CNN_IN_WIDTH,\n",
    "                   config.CNN_IN_CH))\n",
    "        tf_train_labels = tf.placeholder(\n",
    "            tf.float32, shape=(config.BATCH_SIZE, model.NUM_CLASSES))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # Training computation\n",
    "        logits = model.cnn(tf_train_dataset, model_params, keep_prob=0.5)\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_sum(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=tf_train_labels))\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        optimizer = tf.train.AdamOptimizer(config.LR).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            model.cnn(tf_valid_dataset, model_params, keep_prob=1.0))\n",
    "        test_prediction = tf.nn.softmax(\n",
    "            model.cnn(tf_test_dataset, model_params, keep_prob=1.0))\n",
    "        # Merge all summaries\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(config.TRAIN_DIR)\n",
    "\n",
    "        # Add ops to save and restore all the variables\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "    #TRAINING\n",
    "    with tf.Session(graph=graph) as sess:   \n",
    "        tf.global_variables_initializer().run()\n",
    "        if initial_weights is not None:\n",
    "            sess.run(assign_ops)\n",
    "            print('initialized by pre-learned values')\n",
    "        else:\n",
    "            print('initialized')\n",
    "        for step in range(settings.MAX_STEPS):\n",
    "### IMPORT DATA HERE###\n",
    "            feed_dict[\"tf_train_dataset\"],feed_dict[\"tf_test_labels\"]\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                ### STEP HERE\n",
    "            except KeyboardInterrupt:\n",
    "                last_weights = [p.eval() for p in model_params]\n",
    "                np.savez(\"weights.npz\", *last_weights)\n",
    "                return last_weights\n",
    "\n",
    "        \n",
    "\n",
    "#SAVE IT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-0cb63971a250>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         tf_train_labels = tf.placeholder(\n\u001b[0;32m     34\u001b[0m             tf.float32, shape=(config.BATCH_SIZE, model.NUM_CLASSES))\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtf_valid_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mtf_test_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
