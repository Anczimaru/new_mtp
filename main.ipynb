{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded libs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import time\n",
    "import config\n",
    "\n",
    "\n",
    "\n",
    "#To change\n",
    "from my_load_data import load_dataset_fn \n",
    "print(\"loaded libs\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###COMMENTS\n",
    "\"\"\"\n",
    "Accuracy makes softmax of whole tensor\n",
    "\n",
    "CHANGE DATA PIPELINE\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "loaded params\n"
     ]
    }
   ],
   "source": [
    "print (len(os.listdir(config.PIC_SRC_DIR)))\n",
    "\n",
    "\n",
    "params = {\"result_dir\": config.RESULT_DIR,\n",
    "          \"learning_rate\": config.LR,\n",
    "          \"img_size\": config.CNN_IN_HEIGHT,\n",
    "          \"num_channels\":config.CNN_IN_CH,\n",
    "          \"num_classes\": config.CNN_OUTPUT_SIZE,\n",
    "          \"batch_size\": config.BATCH_SIZE,\n",
    "          \"total_steps\": 1,\n",
    "          \"keep_probability\": config.KEEP_PROB,\n",
    "          \"print_nth_step\": config.PRINT_NTH}\n",
    "print(\"loaded params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRAPPERS FOR MODEL\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator of decorator, allowing use of lazy property if no arguments are provided\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope = None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Lazy decorator, optimizes code by loading class Model parts only once to memory\n",
    "    Also its groups tf.Graph, in tensorboard into smaller, more readable parts\n",
    "    \"\"\"    \n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "    \n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            #Sorting Graph by Var_scope\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "                print(\"Initialized Model.{}\".format(name))\n",
    "        return getattr(self, attribute)\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Model of NN\n",
    "    \"\"\"\n",
    "    #Variable of model\n",
    "    #accuracy = tf.reduce\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, params):\n",
    "        #self.permutation = np.random.permutation(range(len(os.listdir(config.PIC_SRC_DIR))))\n",
    "        \n",
    "        self.img_size = params[\"img_size\"]\n",
    "        self.num_channels = params[\"num_channels\"]\n",
    "        self.num_classes = params[\"num_classes\"]\n",
    "        self.lr = params[\"learning_rate\"]\n",
    "        self.keep_prob = params[\"keep_probability\"]\n",
    "        self.write_step = params[\"print_nth_step\"]\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
    "                                trainable=False, name='global_step')\n",
    "       \n",
    "        \n",
    "        \n",
    "        self.training = False ### USE IN FUTURE\n",
    "        \n",
    "        #remove bellow\n",
    "        self.data = tf.placeholder(dtype= tf.float32, shape=[None,256,256,3])\n",
    "        self.target = tf.placeholder(dtype = tf.float32, shape=[None,5])\n",
    "        self.permutation = np.random.permutation(range(1,6))\n",
    "        self.data_iter = 1 \n",
    "        self.total_epoch = 0\n",
    "        \n",
    "        #FUNCTIONS, don not remove\n",
    "        self.load_data\n",
    "        self.prediction\n",
    "        self.loss_op\n",
    "        self.optimize\n",
    "   \n",
    "        \n",
    "        \n",
    "    #FUNCTION DEFINITIONS\n",
    "    def data_pipeline(self):\n",
    "        \"\"\"\n",
    "        loading of TFRecords\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #CHANGE LOAD DATA TO DATA_PIPELINE\n",
    "    @define_scope    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Serve random data each iteration\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Input'):\n",
    "            self.data, self.target = (load_dataset_fn(self.permutation[self.data_iter]))\n",
    "            print(type(self.data))\n",
    "            print(self.data.shape)\n",
    "            print(self.target.shape)\n",
    "        \n",
    "   \n",
    "    @define_scope\n",
    "    def prediction(self, if_training=True):\n",
    "        \"\"\"\n",
    "        Main body of neural network, takes data and labels as input,\n",
    "        returns feature map of photo\n",
    "        \"\"\"\n",
    "        #INPUT LAYER\n",
    "        input_layer = tf.reshape(self.data,[-1, self.img_size, self.img_size, self.num_channels]) \n",
    "        \n",
    "        #1 conv layer\n",
    "        conv1 = tf.layers.conv2d(inputs = self.data, \n",
    "                             filters = 16,\n",
    "                             kernel_size = 5,\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu)\n",
    "        \n",
    "        #1 pool layer, img size reduced by 1/4\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\")\n",
    "\n",
    "        #2 conv layer\n",
    "        conv2 = tf.layers.conv2d(inputs = pool1, \n",
    "                             filters = 32,\n",
    "                             kernel_size = 5,\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu)\n",
    "\n",
    "        #2 pool overal image size reduced totaly by factor of 1/16\n",
    "        pool2 = tf.layers.max_pooling2d(inputs = conv2,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\")\n",
    "        \n",
    "        pool2_flat = tf.reshape(pool2,[-1, 64*64*32])\n",
    "\n",
    "        dense = tf.layers.dense(inputs = pool2_flat,\n",
    "                            units = 128,\n",
    "                            activation = tf.nn.relu)\n",
    "\n",
    "        dense2 = tf.layers.dense(inputs = dense,\n",
    "                             units = self.num_classes,\n",
    "                             activation = tf.nn.relu)\n",
    "       \n",
    "        return tf.nn.softmax(dense2)\n",
    "    \n",
    "    \n",
    "    @define_scope\n",
    "    def loss_op(self):\n",
    "        \"\"\"\n",
    "        loss\n",
    "        \"\"\"\n",
    "        self.loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.prediction,\n",
    "                                                                labels = self.target))\n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    @define_scope    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimizer of model\n",
    "        \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        \n",
    "        return optimizer.minimize(self.loss)\n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, sess, init, saver, writer,step, ckpt_dir):\n",
    "        \"\"\"\n",
    "        Training op for model\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        try: \n",
    "            #train\n",
    "            loss, _ = sess.run([self.loss_op, self.optimize])\n",
    "            if (step + 1) % self.write_step == 0:\n",
    "                print('Loss at step {0}: {1}'.format(step, loss))\n",
    "            #save\n",
    "           # writer.add_summary(summary, global_step=step)\n",
    "            step += 1\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "        except KeyboardInterrupt:\n",
    "            saver.save(sess, ckpt_dir, step)\n",
    "            print(\"keyboard interrupt\") \n",
    "        saver.save(sess, ckpt_dir, step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(step, total_loss/num_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 10\n",
    "def main():\n",
    "    result_dir = config.RESULT_DIR\n",
    "    \n",
    "    #Check for dirs, if not present make them\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    ckpt_dir=os.path.join(result_dir,\"ckpt\")\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    #Get name as default graph\n",
    "    with graph.as_default():\n",
    "\n",
    "        print(\"Starting Session\")\n",
    "        #Assign name to session, assign it's default graph as graph\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            \n",
    "            #Creating summary writer \n",
    "            writer = tf.summary.FileWriter(ckpt_dir, graph=graph)\n",
    "                \n",
    "            #Initialization of Model, load all Model functions returning variables\n",
    "            model = Model(params)\n",
    "            \n",
    "            #Assign Initializer\n",
    "            init = tf.global_variables_initializer()\n",
    "            \n",
    "            #Creating save for model session for future saving and restoring model\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            #Loading last checkpoint\n",
    "            ckpt = tf.train.get_checkpoint_state(result_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                #if ckpt found load it and load global step\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print(\"Found checkpoint\")\n",
    "                step = model.gstep\n",
    "            else: step = 1\n",
    "            \n",
    "                    \n",
    "            #Training\n",
    "            print(\"Starting Training\")\n",
    "           \n",
    "            for epoch in range(total_epoch):\n",
    "                step = model.train(sess, init, saver, writer, step, ckpt_dir)\n",
    "              \n",
    "            \n",
    "        \n",
    "        \n",
    "        print(\"Finnished session\")\n",
    "        #Merge all summaries\n",
    "        #writer.flush()\n",
    "        writer.add_graph(graph)\n",
    "        writer.close()\n",
    "        print(\"Closed summary, work finnished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Session\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(20, 256, 256, 3)\n",
      "(20, 5)\n",
      "Initialized Model.load_data\n",
      "Initialized Model.prediction\n",
      "Initialized Model.loss_op\n",
      "Initialized Model.optimize\n",
      "INFO:tensorflow:Restoring parameters from results\\ckpt-2\n",
      "Found checkpoint\n",
      "Starting Training\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "Average loss at epoch Tensor(\"add_1:0\", shape=(), dtype=int32): 18.353199005126953\n",
      "Took: 7.921292543411255 seconds\n",
      "Average loss at epoch Tensor(\"add_3:0\", shape=(), dtype=int32): 20.5350341796875\n",
      "Took: 5.852907419204712 seconds\n",
      "Average loss at epoch Tensor(\"add_5:0\", shape=(), dtype=int32): 16.682226181030273\n",
      "Took: 5.924044847488403 seconds\n",
      "Average loss at epoch Tensor(\"add_7:0\", shape=(), dtype=int32): 19.208091735839844\n",
      "Took: 5.521616458892822 seconds\n",
      "Average loss at epoch Tensor(\"add_9:0\", shape=(), dtype=int32): 21.051912307739258\n",
      "Took: 6.231894254684448 seconds\n",
      "Average loss at epoch Tensor(\"add_11:0\", shape=(), dtype=int32): 15.79419231414795\n",
      "Took: 6.030658960342407 seconds\n",
      "Average loss at epoch Tensor(\"add_13:0\", shape=(), dtype=int32): 18.99359703063965\n",
      "Took: 6.690356016159058 seconds\n",
      "Average loss at epoch Tensor(\"add_15:0\", shape=(), dtype=int32): 13.929883003234863\n",
      "Took: 6.31891655921936 seconds\n",
      "Average loss at epoch Tensor(\"add_17:0\", shape=(), dtype=int32): 19.03043556213379\n",
      "Took: 5.623260498046875 seconds\n",
      "Average loss at epoch Tensor(\"add_19:0\", shape=(), dtype=int32): 19.613908767700195\n",
      "Took: 5.53231143951416 seconds\n",
      "Average loss at epoch Tensor(\"add_21:0\", shape=(), dtype=int32): 16.4288387298584\n",
      "Took: 5.301091432571411 seconds\n",
      "Average loss at epoch Tensor(\"add_23:0\", shape=(), dtype=int32): 19.109113693237305\n",
      "Took: 6.117288589477539 seconds\n",
      "Average loss at epoch Tensor(\"add_25:0\", shape=(), dtype=int32): 12.21107006072998\n",
      "Took: 6.2092766761779785 seconds\n",
      "Average loss at epoch Tensor(\"add_27:0\", shape=(), dtype=int32): 11.246623992919922\n",
      "Took: 6.99835205078125 seconds\n",
      "Average loss at epoch Tensor(\"add_29:0\", shape=(), dtype=int32): 13.907564163208008\n",
      "Took: 6.13350510597229 seconds\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function WritableFile_Close> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m     \u001b[0m__setattr__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b406b2fcc0d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m      \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-21e68191719b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-488b638f8125>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sess, init, saver, writer, step, ckpt_dir)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"keyboard interrupt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Average loss at epoch {0}: {1}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Took: {0} seconds'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1675\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m           self.export_meta_graph(\n\u001b[1;32m-> 1677\u001b[1;33m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[0;32m   1678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1720\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m         \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1722\u001b[1;33m         strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[0;32m   1723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1724\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m       **kwargs)\n\u001b[0m\u001b[0;32m   2057\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[1;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         as_text=as_text)\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[1;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[0;32m     71\u001b[0m                                         text_format.MessageToString(graph_def))\n\u001b[0;32m     72\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[1;34m(filename, contents, overwrite)\u001b[0m\n\u001b[0;32m    432\u001b[0m   \"\"\"\n\u001b[0;32m    433\u001b[0m   \u001b[0mtemp_pathname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".tmp\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m   \u001b[0mwrite_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mwrite_string_to_file\u001b[1;34m(filename, file_content)\u001b[0m\n\u001b[0;32m    312\u001b[0m   \"\"\"\n\u001b[0;32m    313\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    206\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;34m\"\"\"Make usable with \"with\" statement.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mret_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSet_TF_Status_from_Status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mClose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mClose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1656\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWritableFile_Close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function WritableFile_Close> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "if __name__ == '__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
