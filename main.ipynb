{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded libs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import time\n",
    "import config\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "#To change\n",
    "from my_load_data import load_dataset_fn \n",
    "print(\"loaded libs\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###COMMENTS\n",
    "\"\"\"\n",
    "Accuracy makes softmax of whole tensor\n",
    "\n",
    "Find proper l2 loss function(IOU minimalization)\n",
    "CHANGE DATA PIPELINE!!!!!!\n",
    "get ssh scrypt\n",
    "upload prepared dataset\n",
    "change labels to l2 coordinates from (x,y,angle,scale)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "loaded params\n"
     ]
    }
   ],
   "source": [
    "print (len(os.listdir(config.PIC_SRC_DIR)))\n",
    "\n",
    "\n",
    "params = {\"result_dir\": config.RESULT_DIR,\n",
    "          \"learning_rate\": config.LR,\n",
    "          \"img_size\": config.CNN_IN_HEIGHT,\n",
    "          \"num_channels\":config.CNN_IN_CH,\n",
    "          \"num_classes\": config.CNN_OUTPUT_SIZE,\n",
    "          \"batch_size\": config.BATCH_SIZE,\n",
    "          \"total_steps\": 1,\n",
    "          \"keep_probability\": config.KEEP_PROB,\n",
    "          \"print_nth_step\": 10}\n",
    "print(\"loaded params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRAPPERS FOR MODEL\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator of decorator, allowing use of lazy property if no arguments are provided\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope = None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Lazy decorator, optimizes code by loading class Model parts only once to memory\n",
    "    Also its groups tf.Graph, in tensorboard into smaller, more readable parts\n",
    "    \"\"\"    \n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "    \n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            #Sorting Graph by Var_scope\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "                print(\"Initialized Model.{}\".format(name))\n",
    "        return getattr(self, attribute)\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Model of NN\n",
    "    \"\"\"\n",
    "    #Variable of model\n",
    "    #accuracy = tf.reduce\n",
    "    \n",
    "    \n",
    "### INITIALIZATION ####    \n",
    "    def __init__(self, params):\n",
    "        #self.permutation = np.random.permutation(range(len(os.listdir(config.PIC_SRC_DIR))))\n",
    "        \n",
    "        self.img_size = params[\"img_size\"]\n",
    "        self.num_channels = params[\"num_channels\"]\n",
    "        self.num_classes = params[\"num_classes\"]\n",
    "        self.lr = params[\"learning_rate\"]\n",
    "        self.keep_prob = params[\"keep_probability\"]\n",
    "        self.write_step = params[\"print_nth_step\"]\n",
    "        self.is_training = False\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        #remove bellow\n",
    "        self.data = tf.placeholder(dtype= tf.float32, shape=[None,256,256,3])\n",
    "        self.target = tf.placeholder(dtype = tf.float32, shape=[None,5])\n",
    "        self.permutation = np.random.permutation(range(1,6))\n",
    "        self.data_iter = 1 \n",
    "        self.total_epoch = 0\n",
    "        \n",
    "        #FUNCTIONS, don not remove\n",
    "        self.load_data\n",
    "        self.prediction\n",
    "        self.loss_op\n",
    "        self.optimize\n",
    "        self.summary\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "#FUNCTION DEFINITIONS\n",
    "    def data_pipeline(self):\n",
    "        \"\"\"\n",
    "        loading of TFRecords\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "#CHANGE LOAD DATA TO DATA_PIPELINE\n",
    "    @define_scope    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Serve random data each iteration\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Input'):\n",
    "            self.data, self.target = (load_dataset_fn(self.permutation[self.data_iter]))\n",
    "            #print(type(self.data))\n",
    "            #print(self.data.shape)\n",
    "            #print(self.target.shape)\n",
    "            #self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "            #self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n",
    "            \n",
    "            \n",
    "##### NEURAL NETWORK #####   \n",
    "    @define_scope\n",
    "    def prediction(self, if_training=True):\n",
    "        \"\"\"\n",
    "        Main body of neural network, takes data and labels as input,\n",
    "        returns feature map of photo\n",
    "        \"\"\"\n",
    "        #INPUT LAYER\n",
    "        input_layer = tf.reshape(self.data,[-1, self.img_size, self.img_size, self.num_channels]) \n",
    "        \n",
    "        #1 conv layer\n",
    "        conv1 = tf.layers.conv2d(inputs = self.data, \n",
    "                             filters = 32,\n",
    "                             kernel_size = 5,\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu)\n",
    "        \n",
    "        #1 pool layer, img size reduced by 1/4\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\")\n",
    "\n",
    "        #2 conv layer\n",
    "        conv2 = tf.layers.conv2d(inputs = pool1, \n",
    "                             filters = 64,\n",
    "                             kernel_size = 5,\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu)\n",
    "\n",
    "        #2 pool overal image size reduced totaly by factor of 1/16\n",
    "        pool2 = tf.layers.max_pooling2d(inputs = conv2,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\")\n",
    "        \n",
    "        pool2_flat = tf.reshape(pool2,[-1, 64*64*64])\n",
    "\n",
    "        dense = tf.layers.dense(inputs = pool2_flat,\n",
    "                            units = 128,\n",
    "                            activation = tf.nn.relu)\n",
    "        dropout = tf.layers.dropout(dense,rate = self.keep_prob,training = self.is_training)\n",
    "        \n",
    "        dense2 = tf.layers.dense(inputs = dropout,\n",
    "                             units = self.num_classes,\n",
    "                             activation = tf.nn.relu)\n",
    "       \n",
    "        return tf.nn.softmax(dense2)\n",
    "    \n",
    "    \n",
    "    \n",
    "#### LOSS ####    \n",
    "    @define_scope\n",
    "    def loss_op(self):\n",
    "        \"\"\"\n",
    "        loss\n",
    "        \"\"\"\n",
    "        self.loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.prediction,\n",
    "                                                                labels = self.target))\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    \n",
    "##### OPTIMIZER #####    \n",
    "    @define_scope    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimizer of model\n",
    "        \"\"\"\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        \n",
    "        return optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "    \n",
    "    \n",
    "    \n",
    "###summary####    \n",
    "    @define_scope\n",
    "    def summary(self):\n",
    "        '''\n",
    "        Create summaries to write on TensorBoard\n",
    "        '''\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.histogram('histogram_loss', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "            \n",
    "            \n",
    "#### TRAIN ###        \n",
    "    def train(self, sess, ckpt_dir, writer, saver, step, epoch):\n",
    "        \"\"\"\n",
    "        Training op for model\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.is_training = True\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        self.training = True\n",
    "        try :\n",
    "                #train\n",
    "                loss, _,summary = sess.run([self.loss_op, self.optimize, self.summary_op])\n",
    "                writer.add_summary(summary, global_step=step)\n",
    "                if (step + 1) % self.write_step == 0:\n",
    "                    print('Loss at step {0}: {1}'.format(step, loss))\n",
    "                    saver.save(sess, ckpt_dir, global_step = step)\n",
    "                #save\n",
    "                # writer.add_summary(summary, global_step=step)\n",
    "                self.data_iter+=1\n",
    "                step += 1\n",
    "                total_loss += loss\n",
    "                num_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/num_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        \n",
    "        return step\n",
    "    \n",
    "    \n",
    "    \n",
    " #### EVALUATE ####    \n",
    "    def evaluate(self, init, step, epoch):\n",
    "        \"\"\"\n",
    "        Evaluate once from test\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.is_training = False\n",
    "        try:\n",
    "            loss, summary = sess.run([self.loss, self.summary_op])\n",
    "            writer.add_summary(summary, global_step=step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Loss at epoch {0}: {1}'.format(epoch, loss))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        \n",
    "        \n",
    "        \n",
    " ### TO DO ####   \n",
    "    def train_n_times(self, n_times):\n",
    "        pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 7\n",
    "def main(remove_results = False):\n",
    "    result_dir = config.RESULT_DIR\n",
    "    \n",
    "    if remove_results == True:\n",
    "        shutil.rmtree(result_dir, ignore_errors=True)\n",
    "    #Check for dirs, if not present make them\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    ckpt_dir=os.path.join(result_dir,\"ckpt\")\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    #Get name as default graph\n",
    "    with graph.as_default():\n",
    "\n",
    "        print(\"Starting Session\")\n",
    "        #Assign name to session, assign it's default graph as graph\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            \n",
    "            #Creating summary writer \n",
    "            writer = tf.summary.FileWriter(ckpt_dir, graph=graph)\n",
    "                \n",
    "            #Initialization of Model, load all Model functions returning variables\n",
    "            model = Model(params)\n",
    "            \n",
    "            #Assign Initializer\n",
    "            init = tf.global_variables_initializer()\n",
    "            \n",
    "            #Creating save for model session for future saving and restoring model\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            sess.run(init)\n",
    "            #Loading last checkpoint\n",
    "            ckpt = tf.train.get_checkpoint_state(result_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                #if ckpt found load it and load global step\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print(\"Found checkpoint\")\n",
    "            step = model.global_step.eval()    \n",
    "                \n",
    "            print(\"we are at step {0}\".format(step))        \n",
    "            #Training\n",
    "            print(\"Starting Training\")\n",
    "\n",
    "            for epoch in range(1,total_epoch):\n",
    "                try: \n",
    "                    step = model.train(sess, ckpt_dir, writer, saver, step, epoch)\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"keyboard interrupt\")\n",
    "                    step = model.global_step.eval()\n",
    "                    break\n",
    "            #model.global_step = tf.convert_to_tensor(step, dtype=tf.int32)\n",
    "            saver.save(sess, ckpt_dir, global_step = step)\n",
    "            print(model.global_step.eval(), step)\n",
    "        print(\"Finnished session\")\n",
    "        #Merge all summaries\n",
    "        #writer.flush()\n",
    "        writer.add_graph(graph)\n",
    "        writer.close()\n",
    "        print(\"Closed summary, work finnished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Session\n",
      "Initialized Model.load_data\n",
      "Initialized Model.prediction\n",
      "Initialized Model.loss_op\n",
      "Initialized Model.optimize\n",
      "Initialized Model.summary\n",
      "we are at step 0\n",
      "Starting Training\n",
      "Average loss at epoch 1: 19.025510787963867\n",
      "Took: 5.471312999725342 seconds\n",
      "Average loss at epoch 2: 19.043148040771484\n",
      "Took: 4.358249187469482 seconds\n",
      "Average loss at epoch 3: 18.963726043701172\n",
      "Took: 4.388251066207886 seconds\n",
      "Average loss at epoch 4: 18.961332321166992\n",
      "Took: 4.302246332168579 seconds\n",
      "Average loss at epoch 5: 18.964054107666016\n",
      "Took: 4.333247661590576 seconds\n",
      "Average loss at epoch 6: 18.964054107666016\n",
      "Took: 4.3522491455078125 seconds\n",
      "6 6\n",
      "Finnished session\n",
      "Closed summary, work finnished\n",
      "Starting Session\n",
      "Initialized Model.load_data\n",
      "Initialized Model.prediction\n",
      "Initialized Model.loss_op\n",
      "Initialized Model.optimize\n",
      "Initialized Model.summary\n",
      "INFO:tensorflow:Restoring parameters from results\\ckpt-6\n",
      "Found checkpoint\n",
      "we are at step 6\n",
      "Starting Training\n",
      "Average loss at epoch 1: -8.518839836120605\n",
      "Took: 5.496314287185669 seconds\n",
      "Average loss at epoch 2: -8.518839836120605\n",
      "Took: 4.273244380950928 seconds\n",
      "Average loss at epoch 3: -8.518839836120605\n",
      "Took: 4.318247079849243 seconds\n",
      "Loss at step 9: -8.518839836120605\n",
      "Average loss at epoch 4: -8.518839836120605\n",
      "Took: 11.82767629623413 seconds\n",
      "Average loss at epoch 5: -8.518839836120605\n",
      "Took: 4.3592493534088135 seconds\n",
      "Average loss at epoch 6: -8.518839836120605\n",
      "Took: 4.583262205123901 seconds\n",
      "12 12\n",
      "Finnished session\n",
      "Closed summary, work finnished\n"
     ]
    }
   ],
   "source": [
    "### tf.reset_default_graph()\n",
    "if __name__ == '__main__':\n",
    "    check = True\n",
    "    main(remove_results=check)\n",
    "    check = False\n",
    "    main(remove_results=check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
