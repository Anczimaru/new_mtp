{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded libs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import time\n",
    "import config\n",
    "import shutil\n",
    "from my_tools import define_scope\n",
    "import my_tools\n",
    "\n",
    "print(\"loaded libs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "params"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "loaded params\n"
     ]
    }
   ],
   "source": [
    "print (len(os.listdir(config.PIC_SRC_DIR)))\n",
    "\n",
    "\n",
    "params = {\"result_dir\": config.RESULT_DIR,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"img_size\": config.CNN_IN_HEIGHT,\n",
    "          \"num_channels\":config.CNN_IN_CH,\n",
    "          \"num_classes\": config.CNN_OUTPUT_SIZE,\n",
    "          \"batch_size\": config.BATCH_SIZE,\n",
    "          \"keep_probability\": config.KEEP_PROB,\n",
    "          \"print_nth_step\": config.PRINT_NTH}\n",
    "print(\"loaded params\")\n",
    "\n",
    "\n",
    "def log_dir_name(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, activation):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./19_logs/lr_{0:.0e}_layers_{1}_nodes_{2}_{3}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(learning_rate,\n",
    "                       num_dense_layers,\n",
    "                       num_dense_nodes,\n",
    "                       activation)\n",
    "\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parser"
    ]
   },
   "outputs": [],
   "source": [
    "def parser(serialized_data):\n",
    "    \"\"\"\n",
    "    Decode and map data properly\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {\"image\" : tf.FixedLenFeature((),tf.string),\n",
    "                   \"class\" : tf.FixedLenFeature((),tf.int64),\n",
    "                   \"x1\" : tf.FixedLenFeature((),tf.float32),\n",
    "                   \"y1\" : tf.FixedLenFeature((),tf.float32),\n",
    "                   \"x2\" : tf.FixedLenFeature((),tf.float32),\n",
    "                   \"y2\" : tf.FixedLenFeature((),tf.float32),\n",
    "                   \"index\" : tf.FixedLenFeature((),tf.int64)}\n",
    "    parsed_features = tf.parse_single_example(serialized_data, features)\n",
    "    \n",
    "    image_string = parsed_features[\"image\"]\n",
    "    image = tf.decode_raw(image_string, tf.uint8)\n",
    "    image = tf.cast(image,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    label = tf.convert_to_tensor((tf.cast(parsed_features[\"class\"], tf.float32),\n",
    "                     parsed_features[\"x1\"],\n",
    "                     parsed_features[\"y1\"],\n",
    "                     parsed_features[\"x2\"],\n",
    "                     parsed_features[\"y2\"]))\n",
    "    index = tf.cast(parsed_features[\"index\"], tf.int32)\n",
    "    \n",
    "    return image, label, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "Network"
    ]
   },
   "outputs": [],
   "source": [
    "#WRAPPERS FOR MODEL\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Model of NN\n",
    "    \"\"\"\n",
    "    #Variable of model\n",
    "    #accuracy = tf.reduce\n",
    "    \n",
    "    \n",
    "### INITIALIZATION ####    \n",
    "    def __init__(self, params):\n",
    "        #self.permutation = np.random.permutation(range(len(os.listdir(config.PIC_SRC_DIR))))\n",
    "        \n",
    "        self.img_size = params[\"img_size\"]\n",
    "        self.num_channels = params[\"num_channels\"]\n",
    "        self.num_classes = params[\"num_classes\"]\n",
    "        self.lr = params[\"learning_rate\"]\n",
    "        self.keep_prob = params[\"keep_probability\"]\n",
    "        self.write_step = 10#params[\"print_nth_step\"]\n",
    "        self.is_training = True\n",
    "        self.global_step = tf.Variable(0, False, dtype = tf.int32, name=\"global_step\")\n",
    "        #remove bellow\n",
    "        #self.data = tf.placeholder(dtype= tf.float32, shape=[None,256,256,3])\n",
    "        #self.target = tf.placeholder(dtype = tf.float32, shape=[None,5])\n",
    "        #self.index = tf.placeholder(dtype= tf.int32,shape=[None,1])\n",
    "\n",
    "        #FUNCTIONS\n",
    "        self.data_pipeline\n",
    "        self.prediction\n",
    "        self.loss_op\n",
    "        self.optimize\n",
    "        self.summary\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "#FUNCTION DEFINITIONS\n",
    "    @define_scope\n",
    "    def data_pipeline(self):\n",
    "        \"\"\"\n",
    "        loading of TFRecords\n",
    "        \"\"\"\n",
    "        train_dataset = tf.data.TFRecordDataset(os.path.join(config.DATA_DIR,config.TFRECORD_NAMES[0]))\n",
    "        test_dataset = tf.data.TFRecordDataset(os.path.join(config.DATA_DIR,config.TFRECORD_NAMES[1]))\n",
    "        train_dataset =train_dataset.map(parser)\n",
    "        test_dataset = test_dataset.map(parser)\n",
    "        \n",
    "        iterator = tf.data.Iterator.from_structure( train_dataset.output_types,\n",
    "                                                    train_dataset.output_shapes)\n",
    "        \n",
    "        self.data, target, self.index = iterator.get_next()\n",
    "        self.target = tf.reshape(target, shape = [5])\n",
    "        self.data = tf.reshape(self.data,[-1, self.img_size, self.img_size, self.num_channels]) \n",
    "        \n",
    "        self.train_init = iterator.make_initializer(train_dataset)\n",
    "        #test_init = iterator.make_initializer(test_dataset)\n",
    "\n",
    "        \n",
    "   \n",
    " ##### NEURAL NETWORK #####   \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        \"\"\"\n",
    "        Main body of neural network, takes data and labels as input,\n",
    "        returns feature map of photo\n",
    "        \"\"\"\n",
    "    \n",
    "        #1 conv layer\n",
    "        conv1 = tf.layers.conv2d(inputs = self.data, \n",
    "                             filters = 32,\n",
    "                             kernel_size = [5,5],\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu,\n",
    "                             name = 'conv1')\n",
    "        \n",
    "        #1 pool layer, img size reduced by 1/4\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\",\n",
    "                                        name = 'pool1')\n",
    "\n",
    "        #2 conv layer\n",
    "        conv2 = tf.layers.conv2d(inputs = pool1, \n",
    "                             filters = 64,\n",
    "                             kernel_size = 5,\n",
    "                             strides = 1,\n",
    "                             padding = \"same\",\n",
    "                             activation = tf.nn.relu,\n",
    "                             name = 'conv2')\n",
    "\n",
    "        #2 pool overal image size reduced totaly by factor of 1/16\n",
    "        pool2 = tf.layers.max_pooling2d(inputs = conv2,\n",
    "                                        pool_size = 2, \n",
    "                                        strides = 2,\n",
    "                                        padding = \"same\",\n",
    "                                        name = 'pool2')\n",
    "        \n",
    "        \n",
    "        pool2_flat = tf.reshape(pool2,[-1, 64*64*64])\n",
    "\n",
    "        \n",
    "        dense = tf.layers.dense(inputs = pool2_flat,\n",
    "                            units = 512,\n",
    "                            activation = tf.nn.relu,\n",
    "                            name = 'fc')\n",
    "        \n",
    "        dropout = tf.layers.dropout(dense,\n",
    "                                    rate = self.keep_prob,\n",
    "                                    training = self.is_training,\n",
    "                                    name = 'dropout')\n",
    "        \n",
    "        dense2 = tf.layers.dense(inputs = dropout,\n",
    "                             units = self.num_classes,\n",
    "                             activation = tf.nn.relu,\n",
    "                             name = \"logits\")\n",
    "        \n",
    "        return tf.nn.softmax(tf.reshape(dense2, shape=[5]))\n",
    "    \n",
    "#### LOSS ####    \n",
    "    @define_scope\n",
    "    def loss_op(self):\n",
    "        \"\"\"\n",
    "        loss\n",
    "        \"\"\"\n",
    "        self.loss = tf.losses.mean_squared_error(labels = self.target, predictions = self.prediction)\n",
    "    \n",
    "        return self.loss\n",
    "    \n",
    "##### OPTIMIZER #####    \n",
    "    @define_scope    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimizer of model\n",
    "        \"\"\"\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "        a = tf.cast(1,tf.int32) #trash line so fetcher doesnt return errors\n",
    "        return a\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "###summary####    \n",
    "    @define_scope\n",
    "    def summary(self):\n",
    "        '''\n",
    "        Create summaries to write on TensorBoard\n",
    "        '''\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.histogram('histogram_loss', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "            \n",
    "            \n",
    "#### TRAIN ###        \n",
    "    def train_one_time(self, sess, ckpt_dir, writer, saver, step, epoch):\n",
    "        \"\"\"\n",
    "        Training op for model\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        sess.run(self.train_init)\n",
    "        self.is_training = True\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        self.training = True\n",
    "        try :\n",
    "            while True:\n",
    "                #train\n",
    "                l, _,summary = sess.run([self.loss_op, self.optimize, self.summary_op])\n",
    "                writer.add_summary(summary, global_step=step)\n",
    "                if (step) % self.write_step == 0:\n",
    "                    print('Loss at step {0}: {1}'.format(step, l))\n",
    "                total_loss += l\n",
    "                num_batches += 1\n",
    "                step+=1\n",
    "                self.global_step = tf.convert_to_tensor(step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"keyboard interrupt\")\n",
    "            pass\n",
    "        saver.save(sess, ckpt_dir, global_step = step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/num_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "    \n",
    "    \n",
    " #### EVALUATE ####    \n",
    "    def evaluate(self, step, epoch):\n",
    "        \"\"\"\n",
    "        Evaluate once from test\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        sess.run(self.val_init())\n",
    "        self.is_training = False\n",
    "        try:\n",
    "            l, summary = sess.run([self.loss_op, self.summary_op])\n",
    "            writer.add_summary(summary, global_step=step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Loss at validaiton epoch {0}: {1}'.format(epoch, l))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        \n",
    "        \n",
    "        \n",
    " ### TO DO ####   \n",
    "    def train_n_times(self, n_times):\n",
    "        pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "main"
    ]
   },
   "outputs": [],
   "source": [
    "total_epoch = 2\n",
    "def main(remove_results = False):\n",
    "    result_dir = config.RESULT_DIR\n",
    "    \n",
    "    if remove_results == True:\n",
    "        shutil.rmtree(result_dir, ignore_errors=True)\n",
    "    #Check for dirs, if not present make them\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    ckpt_dir=os.path.join(result_dir,\"ckpt\")\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    #Get name as default graph\n",
    "    with graph.as_default():\n",
    "\n",
    "        print(\"Starting Session\")\n",
    "        #Assign name to session, assign it's default graph as graph\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            \n",
    "            #Creating summary writer \n",
    "            writer = tf.summary.FileWriter(ckpt_dir, graph=graph)\n",
    "                \n",
    "            #Initialization of Model, load all Model functions returning variables\n",
    "            model = Model(params)\n",
    "            \n",
    "            #Assign Initializer\n",
    "            init = tf.global_variables_initializer()\n",
    "            \n",
    "            #Creating save for model session for future saving and restoring model\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            #sess.run(init)\n",
    "            #Loading last checkpoint\n",
    "            ckpt = tf.train.get_checkpoint_state(result_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                #if ckpt found load it and load global step\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print(\"Found checkpoint\")\n",
    "                step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])\n",
    "            else: \n",
    "                sess.run(init)\n",
    "                step = 0\n",
    "             \n",
    "           \n",
    "            print(\"Current step: {0}\".format(step))        \n",
    "            #Add graph\n",
    "            writer.add_graph(graph)\n",
    "            #Training\n",
    "            print(\"Starting Training\")\n",
    "            for epoch in range(1,total_epoch):\n",
    "                try: \n",
    "                    step = model.train(sess, ckpt_dir, writer, saver, step, epoch)\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"keyboard interrupt\")\n",
    "                    pass\n",
    "            print(\"Closing session and saving results\")\n",
    "            print(model.global_step.eval(), step)\n",
    "            saver.save(sess, ckpt_dir, global_step = step)\n",
    "        \n",
    "        #Merge all summaries\n",
    "        writer.flush()\n",
    "        #writer.add_graph(graph)\n",
    "        writer.close()\n",
    "        print(\"Closed summary, work finnished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Session\n",
      "Initialized Model.data_pipeline\n",
      "Initialized Model.prediction\n",
      "Initialized Model.loss_op\n",
      "Initialized Model.optimize\n",
      "Initialized Model.summary\n",
      "Current step: 0\n",
      "Starting Training\n",
      "Loss at step 0: 0.252532958984\n",
      "Loss at step 10: 0.215399175882\n",
      "Loss at step 20: 0.386651605368\n",
      "Loss at step 30: 0.466766357422\n",
      "Loss at step 40: 0.317413300276\n",
      "Loss at step 50: 0.402007550001\n",
      "Average loss at epoch 1: 0.439363815884\n",
      "Took: 13.6275708675 seconds\n",
      "Closing session and saving results\n",
      "(60, 60)\n",
      "Closed summary, work finnished\n",
      "Starting Session\n",
      "Initialized Model.data_pipeline\n",
      "Initialized Model.prediction\n",
      "Initialized Model.loss_op\n",
      "Initialized Model.optimize\n",
      "Initialized Model.summary\n",
      "INFO:tensorflow:Restoring parameters from results/ckpt-60\n",
      "Found checkpoint\n",
      "Current step: 60\n",
      "Starting Training\n",
      "Loss at step 60: 0.444720476866\n",
      "Loss at step 70: 0.412274181843\n",
      "keyboard interrupt\n",
      "Average loss at epoch 1: 0.463223468926\n",
      "Took: 9.27096700668 seconds\n",
      "Closing session and saving results\n",
      "(78, 78)\n",
      "Closed summary, work finnished\n"
     ]
    }
   ],
   "source": [
    "### tf.reset_default_graph()\n",
    "if __name__ == '__main__':\n",
    "    check = True\n",
    "    main(remove_results=check)\n",
    "    check = False\n",
    "    main(remove_results=check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntensorboard --logdir=\"results/\"\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tensorboard --logdir=\"results/\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Session\n",
      "Initialized Model.data_pipeline\n",
      "Initialized Model.prediction\n",
      "Initialized Model.loss_op\n",
      "Initialized Model.optimize\n",
      "Initialized Model.summary\n",
      "INFO:tensorflow:Restoring parameters from results/ckpt-198\n",
      "Found checkpoint\n",
      "Current step: 198\n",
      "Starting Training\n",
      "Loss at step 200: 0.441424578428\n",
      "Loss at step 210: 0.287077844143\n",
      "Loss at step 220: 0.589593529701\n",
      "Loss at step 230: 0.322869867086\n",
      "Loss at step 240: 0.576019287109\n",
      "Loss at step 250: 0.499688714743\n",
      "Average loss at epoch 1: 0.439649326354\n",
      "Took: 15.8380219936 seconds\n",
      "Closing session and saving results\n",
      "(258, 258)\n",
      "Closed summary, work finnished\n",
      "Starting Session\n",
      "Initialized Model.data_pipeline\n",
      "Initialized Model.prediction\n",
      "Initialized Model.loss_op\n",
      "Initialized Model.optimize\n",
      "Initialized Model.summary\n",
      "INFO:tensorflow:Restoring parameters from results/ckpt-258\n",
      "Found checkpoint\n",
      "Current step: 258\n",
      "Starting Training\n",
      "Loss at step 260: 0.596112072468\n",
      "Loss at step 270: 0.51167601347\n",
      "Loss at step 280: 0.589593529701\n",
      "Loss at step 290: 0.571307361126\n",
      "Loss at step 300: 0.576019287109\n",
      "Loss at step 310: 0.160626217723\n",
      "Average loss at epoch 1: 0.427330054343\n",
      "Took: 14.4437630177 seconds\n",
      "Closing session and saving results\n",
      "(318, 318)\n",
      "Closed summary, work finnished\n"
     ]
    }
   ],
   "source": [
    "check = False\n",
    "main(remove_results=check)\n",
    "main(remove_results=check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
